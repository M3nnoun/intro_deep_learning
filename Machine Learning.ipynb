{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is it?\n",
    "\n",
    "- Machine learning (ML) is a techinque which uses computers to discover patterns or information about your data.\n",
    "- It is a part of the wider topic of *artificial intelligence*\n",
    "- There are lots of different types of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples of machine learning\n",
    "\n",
    "- Simplest ML algorithm could be a linear regression. It automatically and iteratively looks at your data to calculate the parameters of your $y = mx + c$ curve\n",
    "- A more advenced technique is *K-means clustering*. It is a way of finding clusters of points in your data without having to input any explicit labels.\n",
    "- The most famous is *neural networks* (NN) which were inspired by the brain and use a directed network of connected neurons to describe features of the data set.\n",
    "    - More recently (since about 2010) *deep neural networks* (DNN) have become possible, allowing more detailed models of data to be learned starting the modern buzz for *deep learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are neural networks\n",
    "\n",
    "Neural netowrks are a collection of ariticial neurons connected together so it's best to start by learning about about neurons.\n",
    "\n",
    "In nature, a neuron is a cell which has an electrical connection to other neurons. If a charge is felt from 'enough' of the input neurons then the neuron fires and passes a charge to its output.\n",
    "\n",
    "An artificial neuron has multiple inputs and can pass its output to multiple other neurons.\n",
    "\n",
    "A neuron will calculate its value, $p = \\sum_i{x_iw_i}$ where $x_i$ is the input value and $w_i$ is a weight assigned to that connection. This $p$ is then passed through some *activation function* to determine the output of the neuron.\n",
    "\n",
    "<img src=\"neuron.png\" alt=\"An artificial neuron\" style=\"width: 200px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Networks\n",
    "\n",
    "The inputs to each neurons either come from the outputs of other neurons or are explicit inputs from the user. This allows you to connect together a large netowrk of neurons:\n",
    "\n",
    "<img src=\"network.png\" alt=\"An artificial neural network\" style=\"width: 400px; margin:0 auto;\"/>\n",
    "\n",
    "In this network every neuron on one layer is connected to every neuron on the next. Every arrow in the diagram has a weight assigned to it.\n",
    "\n",
    "You input values on the left-hand side of the network, and the data flows through the network from layer to layer until the output layer has a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What shape should the network be?\n",
    "\n",
    "There is some art and some science to deciding the shape of a network. There are rules of thumb (hidden layer size should be similar sized to the input and output layers) but this is one of the things that you need to experiment with and see how it affects performance.\n",
    "\n",
    "The number of hidden layers relates to the level of abstraction you are looking at. Generally, more complex problems need more hidden layers (i.e. deeper networks) but this makes training harder.\n",
    "\n",
    "## How are the weights calculated?\n",
    "\n",
    "The calculation of the weights in a network is done through a process called *training*. This generally uses lots of data examples to iteratively work out good values for the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do you train neural networks\n",
    "\n",
    "The main method by which NNs are trained is a technique called *backpropogation*.\n",
    "\n",
    "In order to train your network you need a few things:\n",
    " - A labelled training data set\n",
    " - A labelled test (or evaluation) data set\n",
    " - A set of initial weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Initial weights\n",
    "\n",
    "The weights to start with are easy: just set them randomly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training and testing data sets\n",
    "\n",
    "You will need two data sets. One will be ued by the learning algorithm to train the network and the other will be used to report on the quality of the training at the end.\n",
    "\n",
    "It is important that these data sets are disjoint to prevent *overfitting*.\n",
    "\n",
    "It is common to start with one large set of data that you want to learn about and to split it into 80% training data set and 20% test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation (\"the backward propogation of errors\")\n",
    "\n",
    "Once you have your network structure, your initial weights and your training data set, you can start training.\n",
    "\n",
    "There have been lots of algorithms to do this over the last several decades but the currently most popular one is *backpropogation*.\n",
    "\n",
    "The first thing you need to do is to calculate the derivative of each weight with respect to the output of the network, $D_n = \\frac{dw_n}{dy}$. This gives how much you need to tweak each weight and in which direction to correct the output.\n",
    "\n",
    "Then for each training entry:\n",
    " - pass it through the network and find the value $y$\n",
    " - compare $y$ with the expected true output, $t$ to calculate the error $\\epsilon$\n",
    " - tweak each weight by $\\delta w_n = \\epsilon R \\frac{dw_n}{dy}$ where $R$ is the *learning rate*\n",
    " \n",
    "This means that the 'more wrong' the weights are, the more the move towards the true value. This slows down as, after lots of examples, the network *converges*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"backprop1.png\" alt=\"Back propogation example\" style=\"width: 100%; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Common neural network libraries\n",
    "\n",
    "It would,as with with most things, be possible to to the above by hand but that would take years to make any progress. Instead we use software packages to do the leg work for us.\n",
    "\n",
    "The can in general, construct networks, automatically caluculate derivatives, perform backpropogation and evaluate performance for you.\n",
    "\n",
    "- PyTorch\n",
    "- Tensorflow\n",
    "- Keras\n",
    "- Caffe2\n",
    "- scikit-learn\n",
    "\n",
    "In this workshop, we will be using Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our first neural network: classifying Irises\n",
    "\n",
    "We're going to start with a classic machine learning example, classifying species of Irises.\n",
    "\n",
    "![three iris species](iris_three_species.jpg)\n",
    "\n",
    "Iris setosa, Iris versicolor, and Iris virginica "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data set\n",
    "\n",
    "There [exists a data set of 150 irises](https://en.wikipedia.org/wiki/Iris_flower_data_set), each classified by sepal length and width, and petal length and width.\n",
    "\n",
    "|Sepal length |\tsepal width |\tpetal length |\tpetal width |\tspecies|\n",
    "|--- |\t--- |\t--- |\t--- |\t-|\n",
    "|6.4 |\t2.8 |\t5.6 |\t2.2 |\t2|\n",
    "|5.0 |\t2.3 |\t3.3 |\t1.0 |\t1|\n",
    "|0.9 |\t2.5 |\t4.5 |\t1.7 |\t2|\n",
    "|4.9 |\t3.1 |\t1.5 |\t0.1 |\t0|\n",
    "|... |\t... |\t... |\t... |\t...|\n",
    "\n",
    "Each species label is naturally a string (for example, \"setosa\"), but machine learning typically relies on numeric values. Therefore, someone mapped each string to a number. Here's the representation scheme:\n",
    "\n",
    " - 0 represents setosa\n",
    " - 1 represents versicolor\n",
    " - 2 represents virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The code\n",
    "\n",
    "The Python code that we will be running is available at [premade_estimator.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py). Feel free to follow along with that file but the important parts of the code will be on these slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loading our data\n",
    "\n",
    "Since we're working with a common data set, Tensorflow comes with some helper function to load the data into the correct form for us.\n",
    "\n",
    "In [iris_data.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py), there is a function `load_data`.\n",
    "\n",
    "```python\n",
    ">>> (train_x, train_y), (test_x, test_y) = load_data()\n",
    ">>> train_x.head()\n",
    "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
    "0          6.4         2.8          5.6         2.2\n",
    "1          5.0         2.3          3.3         1.0\n",
    "2          4.9         2.5          4.5         1.7\n",
    "3          4.9         3.1          1.5         0.1\n",
    "4          5.7         3.8          1.7         0.3\n",
    ">>> train_y.head()\n",
    "0    2\n",
    "1    1\n",
    "2    2\n",
    "3    0\n",
    "4    0\n",
    "Name: Species, dtype: int64\n",
    "```\n",
    "It brings in the data from a CSV file into a Pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prepping our data\n",
    "\n",
    "Also in [iris_data.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py) there is a function called `train_input_fn`:\n",
    "\n",
    "```python\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "We pass this `train_x`, `train_y` and our wanted batch size.\n",
    "\n",
    " - First it converts the input data format to a Tensorflow `Dataset`\n",
    " - Then it shuffles, repeats and batches the examples\n",
    " - Finally it returns the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Designing our network\n",
    "\n",
    "Tensorflow comes with a network specially designed for this kind of *classification* problem. It automates a lot of the setup work but has a few configurable parameters.\n",
    "\n",
    "The network is called [tf.estimator.DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) (Deep Neural Network Classifier). In our case we will give it three things:\n",
    " 1. the list of the features (in our case 'SepalLength', 'SepalWidth', 'PetalLength' and 'PetalWidth')\n",
    " 2. the number and size of the hidden layers\n",
    " 3. the number of output classes to create\n",
    "\n",
    "```python\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3\n",
    ")\n",
    "```\n",
    "\n",
    "and that is all that is needed to describe the shape of our network. We can now get to work training it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training our network\n",
    "\n",
    "To train our network, all we need to do is call the `train` method on the classifier object we just created.\n",
    "\n",
    "It takes two arguments: the first is the function to use to generate the training data set so we use our `train_input_fn` from above and the second is the numer of steps to perform which will change how long it trains for.\n",
    "\n",
    "```python\n",
    "classifier.train(\n",
    "    input_fn=lambda:iris_data.train_input_fn(train_x, train_y,\n",
    "                                             args.batch_size),\n",
    "    steps=args.train_steps\n",
    ")\n",
    "```\n",
    "\n",
    "At this point, Tensorflow will go ahead and train the network, outputting its progress to the screen. It should take a few seconds to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluating our model\n",
    "\n",
    "We want to check how good a job the training did so we then evaluate our network on our test data set. It takes a very similar form to training:\n",
    "\n",
    "```python\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,\n",
    "                                            args.batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "```\n",
    "\n",
    "It should print something like:\n",
    "\n",
    "```\n",
    "Test set accuracy: 0.933\n",
    "```\n",
    "\n",
    "telling us that the network classified the test data set with a 93.3% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Use the model\n",
    "\n",
    "Finally, we want to ue the model to make a prediction about the real world. Given a few examples of irises, we evaluate them using the model and compare the results to what would expect:\n",
    "\n",
    "```python\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                            labels=None,\n",
    "                                            batch_size=args.batch_size))\n",
    "```\n",
    "gives us:\n",
    "```\n",
    "Prediction is \"Setosa\" (99.8%), expected \"Setosa\"\n",
    "Prediction is \"Versicolor\" (99.6%), expected \"Versicolor\"\n",
    "Prediction is \"Virginica\" (98.5%), expected \"Virginica\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to image analysis\n",
    "\n",
    "The iris example worked well but the big downside is that it required manual processing of the real-world data before it could be modelled. Someone had to go with a ruler and measure the lengths and widths of each of the flowers. A more common and easily obtainable corpus is images.\n",
    "\n",
    "There have been manay advancements in image analysis but at the core of most of them is *kernel convolution*. This starts by treating the image as a grid of numbers, where each number represents the brightness of the pixel\n",
    "\n",
    "$$\n",
    "\\begin{matrix} \n",
    "105 & 102 & 100 & 97 & 96 & \\dots \\\\\n",
    "103 & 99 & 103 & 101 & 102 & \\dots \\\\\n",
    "101 & 98 & 104 & 102 & 100 & \\dots \\\\\n",
    "99 & 101 & 106 & 104 & 99 & \\dots \\\\\n",
    "104 & 104 & 104 & 100 & 98 & \\dots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{matrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Define a kernel\n",
    "\n",
    "You can then define a *kernel* which defines a filter to be applied to the image:\n",
    "\n",
    "$$\n",
    "Kernel = \\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Depending on the values in the kernel, different filtering operations will be performed. The most common are:\n",
    "\n",
    " - sharpen (shown above)\n",
    " - blur\n",
    " - edge detection (directional or isotropic)\n",
    " \n",
    "The values of the kernels are created by mathematical analysis and are generally fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applying a kernel\n",
    "\n",
    "This kernel is then overlaid over each set of pizels in the image, corresponding values are multiplies and then the total is summed:\n",
    "\n",
    "<img src=\"conv1.jpg\" alt=\"Convolution\" style=\"width: 600px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First pixel\n",
    "\n",
    "![Convolution](conv3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Second pixel\n",
    "\n",
    "![Convolution](conv4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dealing with edges\n",
    "\n",
    "![Convolution](conv5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Before and after\n",
    "\n",
    "If using a Sobel edge detection kernel, you will see the following effect\n",
    "\n",
    "![Before and after](filter.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "At the core of convolutional neural networks is their ability to create abstract feature detectors automatically. If carefully combined, you can create a netowrk which can create layers of abstraction going from \"is there an edge here\" to \"is there an eye here\" to \"is this a person\".\n",
    "\n",
    "From a neural network perspective, there is little different in training. You can simply treat each element of the convolution kernel as a weight as we did before. The backpropogation algorithm will automatically learn the correct values to describe the training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Typical CNN\n",
    "\n",
    "![Typical CNN](typical_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Handwriting recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ethics of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Credits:\n",
    "\n",
    "- Dog photo: CC BY 2.0 [Emily Mathews](https://www.flickr.com/photos/eamathe/14517807267/)\n",
    "- Irises: <a href=\"https://commons.wikimedia.org/w/index.php?curid=170298\"><em>Iris setosa</em></a> (by\n",
    "<a href=\"https://commons.wikimedia.org/wiki/User:Radomil\">Radomil</a>, CC BY-SA 3.0),\n",
    "<a href=\"https://commons.wikimedia.org/w/index.php?curid=248095\"><em>Iris versicolor</em></a> (by\n",
    "<a href=\"https://commons.wikimedia.org/wiki/User:Dlanglois\">Dlanglois</a>, CC BY-SA 3.0),\n",
    "and <a href=\"https://www.flickr.com/photos/33397993@N05/3352169862\"><em>Iris virginica</em></a>\n",
    "(by <a href=\"https://www.flickr.com/photos/33397993@N05\">Frank Mayfield</a>, CC BY-SA\n",
    "2.0).\n",
    "- Kernel convolution images: http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html\n",
    "- CNN layout: CC BY-SA 4.0 [Aphex34](https://commons.wikimedia.org/wiki/File:Typical_cnn.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
